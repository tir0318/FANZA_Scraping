{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FANZAの情報スクレイピング　現在\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sys\n",
    "\n",
    "def download_images(url, excel_path):\n",
    "    # ヘッダーとクッキーの設定\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    cookie = {'age_check_done': '1'}\n",
    "\n",
    "    # 固定のURLリストファイルパス\n",
    "    url_list_path = r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\"\n",
    "\n",
    "    # セッションの初期化\n",
    "    session = requests.Session()\n",
    "    session.get(url, headers=headers, cookies=cookie)\n",
    "\n",
    "    # Excelファイルのロード\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        sheet = wb['記事作成部分']\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"Excelファイルの読み込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "    row_num = 4  # 開始行\n",
    "    try:\n",
    "        with open(url_list_path, 'r', encoding='UTF-8') as f:\n",
    "            urls = f.readlines()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"固定URLリストファイルの読み込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "    for idx, line in enumerate(reversed(urls), start=1):\n",
    "        page_url = line.strip()\n",
    "        print(f\"Processing URL ({idx}): {page_url}\")\n",
    "\n",
    "        try:\n",
    "            # HTMLの取得と解析\n",
    "            response = session.get(page_url, headers=headers, cookies=cookie)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # データの抽出\n",
    "            url_tag = soup.find('meta', property=\"og:url\")\n",
    "            url_title = soup.find(id='title')\n",
    "            name = soup.find(id='performer')\n",
    "            time_tag = soup.find_all('td')\n",
    "\n",
    "            # 要素の存在確認とデータ取得\n",
    "            url_tag_content = url_tag['content'] if url_tag else \"URLが見つかりません\"\n",
    "            url_title_text = url_title.text.strip() if url_title else \"タイトルが見つかりません\"\n",
    "            name_text = name.text.strip() if name else \"出演者情報なし\"\n",
    "            time_text = time_tag[10].text.strip()[:-1] if len(time_tag) > 10 else \"時間情報なし\"\n",
    "\n",
    "            # Excelに書き込み\n",
    "            sheet.cell(row=row_num, column=3, value=url_title_text)\n",
    "            sheet.cell(row=row_num, column=4, value=time_text)\n",
    "            sheet.cell(row=row_num, column=5, value=url_tag_content)\n",
    "            sheet.cell(row=row_num, column=6, value=name_text)\n",
    "\n",
    "            row_num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"URL {page_url} の処理中にエラーが発生しました: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Excelファイルを保存\n",
    "    try:\n",
    "        wb.save(excel_path)\n",
    "        messagebox.showinfo(\"完了\", f\"Excelファイルを保存しました: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"Excelファイルの保存に失敗しました: {e}\")\n",
    "\n",
    "def select_file_and_run(root):\n",
    "    # Excelファイル選択\n",
    "    excel_path = filedialog.askopenfilename(\n",
    "        title=\"Excelファイルを選択してください\",\n",
    "        filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")],\n",
    "        initialdir=os.path.expanduser(\"~/Documents\")\n",
    "    )\n",
    "\n",
    "    if not excel_path:\n",
    "        messagebox.showwarning(\"警告\", \"Excelファイルが選択されていません\")\n",
    "        return\n",
    "\n",
    "    # 確認メッセージ\n",
    "    if not messagebox.askyesno(\"確認\", f\"以下のExcelファイルで処理を開始しますか？\\n\\nExcelファイル: {excel_path}\\n固定URLリスト: C:\\\\Users\\\\yuruy\\\\Downloads\\\\AV\\\\FANZA_URL.txt\"):\n",
    "        return\n",
    "\n",
    "    # GUIを閉じる\n",
    "    root.destroy()\n",
    "\n",
    "    # ダウンロード処理を実行\n",
    "    try:\n",
    "        url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "        download_images(url_certification, excel_path)\n",
    "        sys.exit()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"処理中にエラーが発生しました:\\n{e}\")\n",
    "        sys.exit()\n",
    "\n",
    "def main():\n",
    "    # Tkinterウィンドウを設定\n",
    "    root = tk.Tk()\n",
    "    root.title(\"FANZA情報スクレイピング\")\n",
    "\n",
    "    # ウィンドウを常に最前面に設定\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    # ウィンドウのサイズを設定\n",
    "    root.geometry(\"400x300\")\n",
    "\n",
    "    # 説明ラベル\n",
    "    label = tk.Label(root, text=\"Excelファイルを選択してください\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    # 実行ボタン\n",
    "    button = tk.Button(root, text=\"ファイルを選択して実行\", command=lambda: select_file_and_run(root), font=(\"Arial\", 12), bg=\"lightblue\")\n",
    "    button.pack(pady=40)\n",
    "\n",
    "    # Tkinterメインループ開始\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FANZAの画像スクレイピング　現在\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sys\n",
    "\n",
    "def download_images(url, excel_path):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "    cookie = {'age_check_done': '1'}\n",
    "    session = requests.session()\n",
    "    session.get(url)\n",
    "\n",
    "    # Excelファイルからフォルダ名を取得\n",
    "    wb = openpyxl.load_workbook(excel_path)\n",
    "    sheet = wb.active  # アクティブシートを取得\n",
    "    folder_name = sheet[\"C2\"].value  # C2セルの値を取得\n",
    "    folder_directory = os.path.join(r\"C:\\Users\\yuruy\\Downloads\\AV\", folder_name)\n",
    "\n",
    "    # フォルダ作成\n",
    "    os.makedirs(folder_directory, exist_ok=True)\n",
    "    for i in range(1, 21):  # 1から20の番号でサブフォルダを作成\n",
    "        subfolder_name = str(i).zfill(2)  # \"01\", \"02\", ..., \"20\" の形式\n",
    "        os.makedirs(os.path.join(folder_directory, subfolder_name), exist_ok=True)\n",
    "\n",
    "    with open(r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\", 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        for idx, line in enumerate(reversed(lines), start=1):\n",
    "            soup = BeautifulSoup(session.get(line.strip(), headers=headers, cookies=cookie).content, 'html.parser')\n",
    "            package_image = soup.find('div', class_='center').find('a').get(\"href\")\n",
    "            package_image_name = package_image.split('/')[-1]\n",
    "            print(f\"Downloading package image: {package_image}\")\n",
    "\n",
    "            # フォルダ番号を2桁の形式に変換\n",
    "            subfolder = os.path.join(folder_directory, str((idx - 1) % 20 + 1).zfill(2))  \n",
    "            r = requests.get(package_image)\n",
    "            with open(os.path.join(subfolder, package_image_name), 'wb') as img_file:\n",
    "                img_file.write(r.content)\n",
    "\n",
    "            try:\n",
    "                sample_image_list = soup.find('div', class_='d-zoomimg-sm').find_all('a')\n",
    "                for sample_image in sample_image_list:\n",
    "                    sample_image_url = sample_image.get(\"href\")\n",
    "                    sample_image_name = sample_image_url.split('/')[-1]\n",
    "                    r = requests.get(sample_image_url)\n",
    "                    with open(os.path.join(subfolder, sample_image_name), 'wb') as img_file:\n",
    "                        img_file.write(r.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading sample images for URL {line.strip()}: {e}\")\n",
    "\n",
    "def select_excel_and_run(root):\n",
    "    # ファイル選択ダイアログを開く\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Excelファイルを選択してください\",\n",
    "        filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")],\n",
    "        initialdir=os.path.expanduser(\"~/Documents\")  # 初期ディレクトリをDownloadsに設定\n",
    "    )\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"ファイルが選択されていません\")\n",
    "        return\n",
    "\n",
    "    # 確認メッセージ\n",
    "    if not messagebox.askyesno(\"確認\", f\"選択したファイル: {file_path}\\nこのファイルで処理を開始しますか？\"):\n",
    "        return\n",
    "\n",
    "    # GUIを閉じる\n",
    "    root.destroy()\n",
    "\n",
    "    try:\n",
    "        # ダウンロード処理を実行\n",
    "        url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "        download_images(url_certification, file_path)\n",
    "\n",
    "        # 処理終了メッセージを表示\n",
    "        messagebox.showinfo(\"完了\", \"ダウンロードが完了しました\")\n",
    "\n",
    "        # プログラムを終了\n",
    "        sys.exit()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"処理中にエラーが発生しました:\\n{e}\")\n",
    "        sys.exit()\n",
    "\n",
    "def main():\n",
    "    # Tkinterウィンドウを設定\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Excelファイル選択\")\n",
    "\n",
    "    # ウィンドウを常に最前面に設定\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    # ウィンドウのサイズを設定\n",
    "    root.geometry(\"400x200\")\n",
    "\n",
    "    # 説明ラベル\n",
    "    label = tk.Label(root, text=\"処理に使用するExcelファイルを選択してください\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    # 実行ボタン\n",
    "    button = tk.Button(root, text=\"Excelファイルを選択\", command=lambda: select_excel_and_run(root), font=(\"Arial\", 12), bg=\"lightblue\")\n",
    "    button.pack(pady=20)\n",
    "\n",
    "    # Tkinterメインループ開始\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FANZAの情報スクレイピング\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "def download_images(url):\n",
    "    # ヘッダーとクッキーの設定\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    cookie = {'age_check_done': '1'}\n",
    "\n",
    "    # Excelファイルのパス\n",
    "    excel_path = r\"C:\\Users\\yuruy\\Documents\\納品物\\記事作成依頼_腹筋VR.xlsx\"\n",
    "    url_list_path = r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\"\n",
    "\n",
    "    # セッションの初期化\n",
    "    session = requests.Session()\n",
    "    session.get(url, headers=headers, cookies=cookie)\n",
    "\n",
    "    # Excelファイルのロード\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        sheet = wb['記事作成部分']\n",
    "    except Exception as e:\n",
    "        print(f\"Excelファイルの読み込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "    row_num = 4  # 開始行\n",
    "    with open(url_list_path, 'r', encoding='UTF-8') as f:\n",
    "        urls = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(reversed(urls), start=1):\n",
    "        page_url = line.strip()\n",
    "        print(f\"Processing URL ({idx}): {page_url}\")\n",
    "\n",
    "        try:\n",
    "            # HTMLの取得と解析\n",
    "            response = session.get(page_url, headers=headers, cookies=cookie)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # データの抽出\n",
    "            url_tag = soup.find('meta', property=\"og:url\")\n",
    "            url_title = soup.find(id='title')\n",
    "            name = soup.find(id='performer')\n",
    "            time_tag = soup.find_all('td')\n",
    "\n",
    "            # 要素の存在確認とデータ取得\n",
    "            url_tag_content = url_tag['content'] if url_tag else \"URLが見つかりません\"\n",
    "            url_title_text = url_title.text.strip() if url_title else \"タイトルが見つかりません\"\n",
    "            name_text = name.text.strip() if name else \"出演者情報なし\"\n",
    "            time_text = time_tag[10].text.strip()[:-1] if len(time_tag) > 10 else \"時間情報なし\"\n",
    "\n",
    "            # Excelに書き込み\n",
    "            sheet.cell(row=row_num, column=3, value=url_title_text)\n",
    "            sheet.cell(row=row_num, column=4, value=time_text)\n",
    "            sheet.cell(row=row_num, column=5, value=url_tag_content)\n",
    "            sheet.cell(row=row_num, column=6, value=name_text)\n",
    "\n",
    "            row_num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"URL {page_url} の処理中にエラーが発生しました: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Excelファイルを保存\n",
    "    try:\n",
    "        wb.save(excel_path)\n",
    "        print(f\"Excelファイルを保存しました: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Excelファイルの保存に失敗しました: {e}\")\n",
    "\n",
    "def main():\n",
    "    url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "    download_images(url_certification)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sys\n",
    "\n",
    "def download_images(url, excel_path):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "    cookie = {'age_check_done': '1'}\n",
    "    session = requests.session()\n",
    "    session.get(url)\n",
    "\n",
    "    # Excelファイルからフォルダ名を取得\n",
    "    wb = openpyxl.load_workbook(excel_path)\n",
    "    sheet = wb.active\n",
    "    folder_name = sheet[\"C2\"].value\n",
    "    folder_directory = os.path.join(r\"C:\\Users\\yuruy\\Downloads\\AV\", folder_name)\n",
    "\n",
    "    # フォルダ作成\n",
    "    os.makedirs(folder_directory, exist_ok=True)\n",
    "    for i in range(1, 21):\n",
    "        subfolder_name = str(i).zfill(2)\n",
    "        os.makedirs(os.path.join(folder_directory, subfolder_name), exist_ok=True)\n",
    "\n",
    "    with open(r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\", 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        for idx, line in enumerate(lines):  # reversedを削除し、enumerateのstartも削除\n",
    "            soup = BeautifulSoup(session.get(line.strip(), headers=headers, cookies=cookie).content, 'html.parser')\n",
    "            try:\n",
    "                package_image = soup.find('div', class_='center').find('a').get(\"href\")\n",
    "                package_image_name = package_image.split('/')[-1]\n",
    "                print(f\"Downloading package image: {package_image}\")\n",
    "\n",
    "                subfolder = os.path.join(folder_directory, str((idx % 20) + 1).zfill(2)) # (idx % 20) + 1に変更\n",
    "                r = requests.get(package_image)\n",
    "                with open(os.path.join(subfolder, package_image_name), 'wb') as img_file:\n",
    "                    img_file.write(r.content)\n",
    "\n",
    "                sample_image_list = soup.find('div', class_='d-zoomimg-sm').find_all('a')\n",
    "                for sample_image in sample_image_list:\n",
    "                    sample_image_url = sample_image.get(\"href\")\n",
    "                    sample_image_name = sample_image_url.split('/')[-1]\n",
    "                    r = requests.get(sample_image_url)\n",
    "                    with open(os.path.join(subfolder, sample_image_name), 'wb') as img_file:\n",
    "                        img_file.write(r.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {line.strip()}: {e}\")\n",
    "\n",
    "def select_excel_and_run(root):\n",
    "    # ファイル選択ダイアログを開く\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Excelファイルを選択してください\",\n",
    "        filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")],\n",
    "        initialdir=os.path.expanduser(\"~/Documents\")\n",
    "    )\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"ファイルが選択されていません\")\n",
    "        return\n",
    "\n",
    "    # 確認メッセージ\n",
    "    if not messagebox.askyesno(\"確認\", f\"選択したファイル: {file_path}\\nこのファイルで処理を開始しますか？\"):\n",
    "        return\n",
    "\n",
    "    # GUIを閉じる\n",
    "    root.destroy()\n",
    "\n",
    "    try:\n",
    "        # ダウンロード処理を実行\n",
    "        url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "        download_images(url_certification, file_path)\n",
    "\n",
    "        # 処理終了メッセージを表示\n",
    "        messagebox.showinfo(\"完了\", \"ダウンロードが完了しました\")\n",
    "\n",
    "        # プログラムを終了\n",
    "        sys.exit()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"処理中にエラーが発生しました:\\n{e}\")\n",
    "        sys.exit()\n",
    "\n",
    "def main():\n",
    "    # Tkinterウィンドウを設定\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Excelファイル選択\")\n",
    "\n",
    "    # ウィンドウを常に最前面に設定\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    # ウィンドウのサイズを設定\n",
    "    root.geometry(\"400x200\")\n",
    "\n",
    "    # 説明ラベル\n",
    "    label = tk.Label(root, text=\"処理に使用するExcelファイルを選択してください\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    # 実行ボタン\n",
    "    button = tk.Button(root, text=\"Excelファイルを選択\", command=lambda: select_excel_and_run(root), font=(\"Arial\", 12), bg=\"lightblue\")\n",
    "    button.pack(pady=20)\n",
    "\n",
    "    # Tkinterメインループ開始\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sys\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# 設定ファイルから読み込む定数\n",
    "URL_LIST_PATH = r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\"\n",
    "EXCEL_SHEET_NAME = '記事作成部分'\n",
    "START_ROW = 4\n",
    "TIME_TAG_INDEX = 10\n",
    "\n",
    "\n",
    "def scrape_fanza_data(session: requests.Session, url: str, headers: dict, cookie: dict) -> Optional[Tuple[str, str, str, str]]:\n",
    "    \"\"\"FANZAページから情報をスクレイピングする\"\"\"\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, cookies=cookie)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"URL {url} の取得に失敗しました: {e}\")\n",
    "        return None\n",
    "\n",
    "    url_tag = soup.find('meta', property=\"og:url\")\n",
    "    url_title = soup.find(id='title')\n",
    "    name = soup.find(id='performer')\n",
    "    time_tag = soup.find_all('td')\n",
    "\n",
    "    url_tag_content = url_tag['content'] if url_tag else \"URLが見つかりません\"\n",
    "    url_title_text = url_title.text.strip() if url_title else \"タイトルが見つかりません\"\n",
    "    name_text = name.text.strip() if name else \"出演者情報なし\"\n",
    "    time_text = time_tag[TIME_TAG_INDEX].text.strip()[:-1] if len(time_tag) > TIME_TAG_INDEX else \"時間情報なし\"\n",
    "\n",
    "    return url_title_text, time_text, url_tag_content, name_text\n",
    "\n",
    "\n",
    "def write_to_excel(excel_path: str, data: List[Tuple[str, str, str, str]]):\n",
    "    \"\"\"Excelファイルにデータを書き込む\"\"\"\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        sheet = wb[EXCEL_SHEET_NAME]\n",
    "        for row_num, (title, time, url, name) in enumerate(data, start=START_ROW):\n",
    "            sheet.cell(row=row_num, column=3, value=title)\n",
    "            sheet.cell(row=row_num, column=4, value=time)\n",
    "            sheet.cell(row=row_num, column=5, value=url)\n",
    "            sheet.cell(row=row_num, column=6, value=name)\n",
    "        wb.save(excel_path)\n",
    "        messagebox.showinfo(\"完了\", f\"Excelファイルを保存しました: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"Excelファイルの書き込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "def process_urls(excel_path: str, url_certification: str):\n",
    "    \"\"\"URLリストを読み込み、FANZAからデータをスクレイピングし、Excelに書き込む\"\"\"\n",
    "     # ヘッダーとクッキーの設定\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    cookie = {'age_check_done': '1'}\n",
    "\n",
    "    # セッションの初期化\n",
    "    session = requests.Session()\n",
    "    session.get(url_certification, headers=headers, cookies=cookie)\n",
    "    \n",
    "    try:\n",
    "        with open(URL_LIST_PATH, 'r', encoding='UTF-8') as f:\n",
    "            urls = f.read().splitlines()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"固定URLリストファイルの読み込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "    all_data = []\n",
    "    for idx, url in enumerate(reversed(urls), start=1): #reversedを追加\n",
    "        print(f\"Processing URL ({idx}/{len(urls)}): {url}\")\n",
    "        data = scrape_fanza_data(session, url.strip(), headers, cookie)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "    \n",
    "    write_to_excel(excel_path, all_data)\n",
    "\n",
    "def select_file_and_run(root: tk.Tk):\n",
    "    # Excelファイル選択\n",
    "    excel_path = filedialog.askopenfilename(\n",
    "        title=\"Excelファイルを選択してください\",\n",
    "        filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")],\n",
    "        initialdir=os.path.expanduser(\"~/Documents\")\n",
    "    )\n",
    "\n",
    "    if not excel_path:\n",
    "        messagebox.showwarning(\"警告\", \"Excelファイルが選択されていません\")\n",
    "        return\n",
    "\n",
    "    # 確認メッセージ\n",
    "    if not messagebox.askyesno(\"確認\", f\"以下のExcelファイルで処理を開始しますか？\\n\\nExcelファイル: {excel_path}\\n固定URLリスト: {URL_LIST_PATH}\"):\n",
    "        return\n",
    "\n",
    "    # GUIを閉じる\n",
    "    root.destroy()\n",
    "    try:\n",
    "        url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "        process_urls(excel_path, url_certification)\n",
    "        sys.exit()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"エラー\", f\"処理中にエラーが発生しました:\\n{e}\")\n",
    "        sys.exit()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Tkinterウィンドウを設定\n",
    "    root = tk.Tk()\n",
    "    root.title(\"FANZA情報スクレイピング\")\n",
    "\n",
    "    # ウィンドウを常に最前面に設定\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    # ウィンドウのサイズを設定\n",
    "    root.geometry(\"400x300\")\n",
    "\n",
    "    # 説明ラベル\n",
    "    label = tk.Label(root, text=\"Excelファイルを選択してください\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    # 実行ボタン\n",
    "    button = tk.Button(root, text=\"ファイルを選択して実行\", command=lambda: select_file_and_run(root), font=(\"Arial\", 12), bg=\"lightblue\")\n",
    "    button.pack(pady=40)\n",
    "\n",
    "    # Tkinterメインループ開始\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#名前だけ抽出　現在\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "def download_images(url):\n",
    "    # ヘッダーとクッキーの設定\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    cookie = {'age_check_done': '1'}\n",
    "\n",
    "    # Excelファイルのパス\n",
    "    excel_path = r\"C:\\Users\\yuruy\\Documents\\納品物\\記事作成依頼_色白美肌VR.xlsx\"\n",
    "    url_list_path = r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\"\n",
    "\n",
    "    # セッションの初期化\n",
    "    session = requests.Session()\n",
    "    session.get(url, headers=headers, cookies=cookie)\n",
    "\n",
    "    # Excelファイルのロード\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        sheet = wb['記事作成部分']\n",
    "    except Exception as e:\n",
    "        print(f\"Excelファイルの読み込みに失敗しました: {e}\")\n",
    "        return\n",
    "\n",
    "    row_num = 4  # 開始行\n",
    "    with open(url_list_path, 'r', encoding='UTF-8') as f:\n",
    "        urls = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(reversed(urls), start=1):\n",
    "        page_url = line.strip()\n",
    "        #print(f\"Processing URL ({idx}): {page_url}\")\n",
    "\n",
    "        try:\n",
    "            # HTMLの取得と解析\n",
    "            response = session.get(page_url, headers=headers, cookies=cookie)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # データの抽出\n",
    "            url_tag = soup.find('meta', property=\"og:url\")\n",
    "            url_title = soup.find(id='title')\n",
    "            name = soup.find(id='performer')\n",
    "            time_tag = soup.find_all('td')\n",
    "\n",
    "            # 要素の存在確認とデータ取得\n",
    "            url_tag_content = url_tag['content'] if url_tag else \"URLが見つかりません\"\n",
    "            url_title_text = url_title.text.strip() if url_title else \"タイトルが見つかりません\"\n",
    "            name_text = name.text.strip() if name else \"出演者情報なし\"\n",
    "            time_text = time_tag[10].text.strip()[:-1] if len(time_tag) > 10 else \"時間情報なし\"\n",
    "\n",
    "            print(name_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"URL {page_url} の処理中にエラーが発生しました: {e}\")\n",
    "            continue\n",
    "\n",
    "def main():\n",
    "    url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "    download_images(url_certification)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def process_directory(path):\n",
    "    for lp in os.listdir(path):\n",
    "        list_path = os.path.join(path, lp)\n",
    "\n",
    "        if os.path.isdir(list_path):\n",
    "            file_list = [f for f in os.listdir(list_path) if os.path.isfile(os.path.join(list_path, f))]\n",
    "            if file_list:\n",
    "                process_files(list_path, file_list)\n",
    "\n",
    "def process_files(list_path, file_list):\n",
    "    get_list = [f for f in file_list if '【' in f]\n",
    "    year_list = [re.sub(r\"\\D\", \"\", f)[:4] for f in get_list]\n",
    "\n",
    "    if not year_list:\n",
    "        return\n",
    "\n",
    "    year = int(year_list[0])\n",
    "    process_year_files(list_path, get_list, year)\n",
    "\n",
    "def process_year_files(list_path, get_list, year):\n",
    "    January = datetime.date(year, 1, 1)\n",
    "    last_month = January + relativedelta(months=12, days=-1)\n",
    "    j = 0\n",
    "\n",
    "    while True:\n",
    "        three_month = January + relativedelta(months=3+j, days=-1)\n",
    "        process_quarter(list_path, get_list, January, three_month)\n",
    "\n",
    "        if three_month >= last_month:\n",
    "            print(\"終わり\")\n",
    "            break\n",
    "\n",
    "        j += 3\n",
    "\n",
    "def process_quarter(list_path, get_list, start_date, end_date):\n",
    "    name_list = []\n",
    "    remove_list = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        formatted_date = current_date.strftime('【%Y_%m_%d】')\n",
    "        name_list.append(formatted_date)\n",
    "\n",
    "        file_in = [s for s in get_list if formatted_date in s]\n",
    "        for file_name in file_in:\n",
    "            remove_list.append(os.path.join(list_path, file_name))\n",
    "\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "\n",
    "    if name_list:\n",
    "        try:\n",
    "            os.makedirs(os.path.join(list_path, f'{name_list[0]}～{name_list[-1]}'))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        for file_path in remove_list:\n",
    "            shutil.move(file_path, os.path.join(list_path, f'{name_list[0]}～{name_list[-1]}'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = r\"C:\\Users\\Charlocco\\Desktop\\Browsing_folder\"\n",
    "    process_directory(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "folder_Directory = r\"C:\\Users\\yuruy\\Downloads\\AV\\サンプル\\\\\"\n",
    "folder_Directory = folder_Directory + \"\\\\*\"\n",
    "f_number = 1\n",
    "s_number = 0\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "cookie = {'age_check_done': '1'}\n",
    "url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "session = requests.session()\n",
    "session.get(url_certification) \n",
    "with open(r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\", 'r', encoding='UTF-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in reversed(lines):       \n",
    "        soup = BeautifulSoup(session.get(\"https://www.dmm.co.jp/digital/videoa/-/detail/=/cid=dvaj00262/?i3_ref=search&i3_ord=39\".replace('\\n', ''), headers=headers, cookies=cookie).content, 'html.parser')\n",
    "        package_image = soup.find('div', class_='center').find('a').get_attribute_list('href')[0]\n",
    "        package_image_name = package_image.split('/')[-1]\n",
    "        print(package_image)\n",
    "        r = requests.get(package_image)\n",
    "        with open(folder_Directory + str(s_number) + str(f_number) + '\\\\' + package_image_name, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        try:\n",
    "            sample_image_list = soup.find('div', class_='d-zoomimg-sm').find_all('a')\n",
    "            for sample_image in sample_image_list:\n",
    "                sample_image = sample_image.get(\"href\")\n",
    "                sample_image_name = sample_image.split('/')[-1]\n",
    "                r = requests.get(sample_image)\n",
    "                with open(folder_Directory + str(s_number) + str(f_number) + '\\\\' + sample_image_name, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            pass\n",
    "        f_number += 1\n",
    "        if f_number == 10:\n",
    "            f_number = 0\n",
    "            s_number += 1\n",
    "        sample_image_list.clear()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "cookie = {'age_check_done': '1'}\n",
    "url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "url_target = 'https://www.dmm.co.jp/digital/videoa/-/actress/=/keyword=a/'\n",
    "\n",
    "session = requests.session()\n",
    "session.get(url_certification)        \n",
    "soup = BeautifulSoup(session.get(url_target, headers=headers, cookies=cookie).content, 'html.parser')\n",
    "elems = soup.find('div', class_='d-sect act-box').find_all('a')\n",
    "for i in elems:\n",
    "    name_text = i.get_text('.').split('出演作品：')[0]\n",
    "    name_text = name_text.split('.')\n",
    "    name_1 = name_text[0]\n",
    "    name_2 = name_text[1]\n",
    "    print(name_1)\n",
    "    #print(name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "\n",
    "def download_images(url, excel_path):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "    cookie = {'age_check_done': '1'}\n",
    "    session = requests.session()\n",
    "    session.get(url)\n",
    "\n",
    "    # Excelファイルからフォルダ名を取得\n",
    "    wb = openpyxl.load_workbook(excel_path)\n",
    "    sheet = wb.active  # アクティブシートを取得\n",
    "    folder_name = sheet[\"C2\"].value  # C2セルの値を取得\n",
    "    folder_directory = os.path.join(r\"C:\\Users\\yuruy\\Downloads\\AV\", folder_name)\n",
    "\n",
    "    # フォルダ作成\n",
    "    os.makedirs(folder_directory, exist_ok=True)\n",
    "    for i in range(1, 21):  # 1から20の番号でサブフォルダを作成\n",
    "        subfolder_name = str(i).zfill(2)  # \"01\", \"02\", ..., \"20\" の形式\n",
    "        os.makedirs(os.path.join(folder_directory, subfolder_name), exist_ok=True)\n",
    "\n",
    "    with open(r\"C:\\Users\\yuruy\\Downloads\\AV\\FANZA_URL.txt\", 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        for idx, line in enumerate(reversed(lines), start=1):\n",
    "            soup = BeautifulSoup(session.get(line.strip(), headers=headers, cookies=cookie).content, 'html.parser')\n",
    "            package_image = soup.find('div', class_='center').find('a').get(\"href\")\n",
    "            package_image_name = package_image.split('/')[-1]\n",
    "            print(f\"Downloading package image: {package_image}\")\n",
    "\n",
    "            # フォルダ番号を2桁の形式に変換\n",
    "            subfolder = os.path.join(folder_directory, str((idx - 1) % 20 + 1).zfill(2))  \n",
    "            r = requests.get(package_image)\n",
    "            with open(os.path.join(subfolder, package_image_name), 'wb') as img_file:\n",
    "                img_file.write(r.content)\n",
    "\n",
    "            try:\n",
    "                sample_image_list = soup.find('div', class_='d-zoomimg-sm').find_all('a')\n",
    "                for sample_image in sample_image_list:\n",
    "                    sample_image_url = sample_image.get(\"href\")\n",
    "                    sample_image_name = sample_image_url.split('/')[-1]\n",
    "                    r = requests.get(sample_image_url)\n",
    "                    with open(os.path.join(subfolder, sample_image_name), 'wb') as img_file:\n",
    "                        img_file.write(r.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading sample images for URL {line.strip()}: {e}\")\n",
    "\n",
    "def main():\n",
    "    excel_path = r\"C:\\Users\\yuruy\\Documents\\納品物\\記事作成依頼_腹筋VR.xlsx\" # Excelファイルのパスを指定\n",
    "    url_certification = 'https://www.dmm.co.jp/age_check/=/declared=yes/?rurl=https%3A%2F%2Fwww.dmm.co.jp%2Ftop%2F'\n",
    "    download_images(url_certification, excel_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
